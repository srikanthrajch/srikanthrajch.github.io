<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 8)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- Basic Page Needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Srikanth Raj Chetupalli | IISc</title>
	<meta name="description" content="">
	<meta name="author" content="sraj" >

	<!-- No image indexing-->
	<meta name="robots" content="noimageindex">
   <!-- Mobile Specific Metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
    ================================================== -->
   <link rel="stylesheet" href="css/default.css">
	<link rel="stylesheet" href="css/layout.css">
	<link rel="stylesheet" href="css/my_layout.css">
   <link rel="stylesheet" href="css/media-queries.css">
   <link rel="stylesheet" href="css/magnific-popup.css">

   <!-- Script
   ================================================== -->
	<script src="js/modernizr.js"></script>

   <!-- Favicons
	================================================== -->
	<link rel="shortcut icon" href="content/iisclogo.jpg" >

</head>

<body>

   <!-- Header
   ================================================== -->
   <header id="home">

      <nav id="nav-wrap">

         <a class="mobile-btn" href="#nav-wrap" title="Show navigation">Show navigation</a>
	      <a class="mobile-btn" href="#" title="Hide navigation">Hide navigation</a>

         <ul id="nav" class="nav">
            <li class="current"><a class="smoothscroll" href="#home">Home</a></li>
            <li><a class="smoothscroll" href="#about">About</a></li>
            <li><a class="smoothscroll" href="#publications">Publications</a></li>
            <li><a class="smoothscroll" href="#news">News</a></li>
            <li><a href="gallery.html">More...</a></li>            
         </ul> <!-- end #nav -->

      </nav> <!-- end #nav-wrap -->

      <div class="row banner">
         <div class="banner-text">
            <h1 class="responsive-headline">Srikanth Raj Chetupalli</h1>
            <h3>I'm a <span>Ph.D Student</span>, and <span>TCS Research Fellow</span> in the Department of Electrical 
            Communication Engineering, Indian Institute of Science, Bangalore. 
            </h3>
            <hr />
         </div>
      </div>

      <p class="scrolldown">
         <a class="smoothscroll" href="#about"><i class="icon-down-circle"></i></a>
      </p>

   </header> <!-- Header End -->


   <!-- About Section
   ================================================== -->
   <section id="about">

      <div class="row">

         <div class="three columns">

            <img class="profile-pic"  src="content/profile.png" alt="Profile pic" />
            
         </div>

         <div class="nine columns main-col">

            <h2>About Me</h2>

					<p>
						I am a Ph.D student in the <a href="http://www.ece.iisc.ernet.in/">
						Department of electrical communication engineering</a> (ECE) at the 
						<a href="http://www.iisc.ernet.in/"> Indian Institute of Science</a> 
						(IISc), since August 2013. I am a member of the <a href="http://www.sagiisc.in/">
						Speech and Audio Group</a> headed by <a href="http://www.sagiisc.in/people/faculty" target="_blank">Prof. T. V. Sreenivas</a>.
					</p>
					<p>
						Prior to joining IISc for Ph.D, I worked with Ikanos Communications India Pvt. Ltd. from July 2011 to 
						July 2013 as a Firmware engineer working on VDSL2 systems. I obtained M.E in Signal Processing from IISc 
						in May 2011, and B.Tech degree in Electronics and Communication Engineering from JNTU, Hyderabad in 
						May 2009.</p>
					<p>
						My research interests are in the areas of speech and audio processing, and machine learning. 
						Specifically, I am interested in multi-channel speech processing, speech source localization, 
						self-localization of microphone arrays, acoustic scene analysis, compressed sensing and sparse 
						signal processing applied to speech and audio.
					</p>					

            <div class="row">

               <div class="columns contact-details">

                  <h2>Contact Details</h2>
                  <hr>
                  <p class="address">
						   <span>Srikanth Raj Chetupalli</span><br>
						   <span>Ph.D Student,<br>
									Dept. of Electrical Communication Engineering,<br>
									Indian Institute of Science, Bangalore.
                     </span><br>
                     <span>email: srajATeceDOTiiscDOTernetDOTin</span>
					   </p>

               </div>

               <div class="columns download">
                  <p>
                     <a href="content/resume.pdf" target="_blank" class="button"><i class="fa fa-download"></i>Download CV</a>
                  </p>
               </div>

            </div> <!-- end row -->

         </div> <!-- end .main-col -->

      </div>		
		
   </section> <!-- About Section End-->

   <!-- publications Section
   ================================================== -->
   <section id="publications">

      <div class="row">
         <div class="twelve columns collapsed">

            <h1>Publications.</h1>
			<hr \>
            <!-- publications-wrapper -->
            <div id="publications-wrapper" class="bgrids-full s-bgrids-full cf">            
				<h3>Journal publications</h3>

          	   	<div class="columns publications-item">
				<div class="item-wrap">
					Srikanth Raj Chetupalli, and T. V. Sreenivas,
                    <a href="#journal-01">
					‘‘Late Reverberation Cancellation Using Bayesian Estimation of Multi-Channel Linear Predictors and Student's t-Source Prior,’’
					</a> IEEE/ACM Transactions on Audio, Speech, and Language Processing, Vol. 27, No. 6, Mar 2019.
				</div>
				</div> <!-- item end -->

          	   	<div class="columns publications-item">
                <div class="item-wrap">
					Srikanth Raj Chetupalli, T. V. Sreenivas,
                	<a href="#letters-01">
					‘‘Joint Bayesian Estimation of Time-Varying LP Parameters and Excitation for Speech,’’
					</a> IEEE Signal Processing Letters, Vol 24, No. 4, April 2017.                       
				</div>
          		</div> <!-- item end -->

				<h3>Conference publications</h3>

          	   	<div class="columns publications-item">
				<div class="item-wrap">
					Srikanth Raj Chetupalli, and T. V. Sreenivas,
                    <a href="#conference-13">
					‘‘Clean speech AE-DNN PSD constraint for MCLP based reverberant speech enhancement,’’
					</a> accepted for presentation at European Signal Processing Conference (EUSIPCO), Coruna, Spain, 2-6 Sep, 2019.
				</div>
				</div> <!-- item end -->

          	   	<div class="columns publications-item">
				<div class="item-wrap">
					Srikanth Raj Chetupalli, Anirban Bhowmick, and T. V. Sreenivas,
                    <a href="#conference-12">
					‘‘Ad-hoc mobile array based audio segmentation using latent variable stochastic model,’’
					</a> accepted for presentation at European Signal Processing Conference (EUSIPCO), Coruna, Spain, 2-6 Sep, 2019.
				</div>
				</div> <!-- item end -->

          	   	<div class="columns publications-item">
				<div class="item-wrap">
					Srikanth Raj Chetupalli, T. V. Sreenivas, and Anand Gopalakrishnan,
                    <a href="#conference-11">
					‘‘Comparison of low-dimension speech segment embeddings: Application to speaker diarization,’’
					</a> National conference on communications (NCC), Bangalore, India, 20-23 Feb, 2019.                       
				</div>
				</div> <!-- item end -->

          	   	<div class="columns publications-item">
				<div class="item-wrap">
					Srikanth Raj Chetupalli, and T. V. Sreenivas,
                    <a href="#conference-10">
					‘‘Linear Prediction Based Diffuse Signal Estimation for Blind Microphone Geometry Calibration,’’
					</a> International workshop on Acoustic signal enhancement (IWAENC), Tokyo, Japan, 17-20 Sep, 2018.
				</div>
				</div> <!-- item end -->                                                
          		            
          	   	<div class="columns publications-item">
				<div class="item-wrap">
					Srikanth Raj Chetupalli, Ashwin Ram and T. V. Sreenivas,
                    <a href="#conference-09">
					‘‘Robust offline trained neural network for TDOA based sound source localization,’’
					</a> National Conference on Communications (NCC), IIT Hyderabad, India, 25-28 Feb, 2018.
                </div>
          		</div> <!-- item end -->                                                
          		
          	   	<div class="columns publications-item">
                <div class="item-wrap">
					Amit Kumar Verma, Hemendra Tomar, Srikanth Raj Chetupalli, and T. V. Sreenivas,
                    <a href="#conference-08">
					‘‘Non-Linear Filtering for Feature Enhancement of Reverberant Speech,’’
					</a> IEEE TECON 2017, Penang, Malaysia, 5-8 Nov, 2017.
                </div>
          		</div> <!-- item end -->

				<div class="columns publications-item">
                <div class="item-wrap">
					Neeraj Sharma, Shreepad Potadar, Srikanth Raj Chetupalli, T. V. Sreenivas,
                	<a href="#conference-07">
					‘‘Mel-Scale Sub-band Modelling for Perceptually improved Time-Scale Modification of Speech and Audio Signals,’’
					</a> National Conference on Communications, Chennai, India, 2-4 Mar, 2017.
                </div>
          		</div> <!-- item end -->                        

          	   	<div class="columns publications-item">
                <div class="item-wrap">
					Srikanth Raj Chetupalli, Anand Gopalakrishnan, and T. V. Sreenivas,
                    <a href="#conference-06">
					‘‘Feature Selection and Model Optimization for Semi-supervised Speaker Spotting,’’
					</a> European Signal Processing Conference (EUSIPCO), Budapest, Hungary, Aug 29-Sep 2, 2016.                       
                </div>
          		</div> <!-- item end -->

				<div class="columns publications-item">
                <div class="item-wrap">
					Srikanth Raj Chetupalli, and T. V. Sreenivas,
                    <a href="#conference-05">
					‘‘Successive Approximation Algorithm for LPC Estimation Using Sparse Residual Constraint,’’
					</a> National Conference on Communication (NCC), Mumbai, India, Feb 27-Mar 1, 2015.                       
                </div>
          		</div> <!-- item end -->

				<div class="columns publications-item">
                <div class="item-wrap">
					Srikanth Raj Chetupalli, and T. V. Sreenivas,
                    <a href="#conference-04">
					‘‘Time Varying Linear Prediction using Sparsity Constraints,’’
					</a> IEEE International Conference on Acoustics, Speech, and Signal Processing, Florence, Italy, 4-9 May, 2014.                       
				</div>
          		</div> <!-- item end -->

          	   	<div class="columns publications-item">
                <div class="item-wrap">
					Ch. Srikanth Raj, and T. V. Sreenivas,
                    <a href="#conference-03">
					‘‘Joint Pitch-Analysis Formant-Synthesis framework for CS recovery of speech,’’
					</a> in Proc. of INTERSPEECH, Portland Oregon, USA, 9-13 Sep, 2012.                      
                </div>
          		</div> <!-- item end -->

				<div class="columns publications-item">
				<div class="item-wrap">
					Ch. Srikanth Raj, and T. V. Sreenivas,
                    <a href="#conference-02">
					‘‘Time-varying signal adaptive transform and IHT recovery of compressive sensed speech,’’
					</a> in Proc. of INTERSPEECH, Florence, Italy, 28-31 Aug, 2011.                       
                </div>
          		</div> <!-- item end -->

				<div class="columns publications-item">
                <div class="item-wrap">
					Ch. Srikanth Raj, and T. V. Sreenivas,
                    <a href="#conference-01">
					‘‘Compressive Sensing for Music signals: Comparison of transforms with coherent dictionaries,’’
					</a> in Proc. of 42nd AES International Conference, Ilmenau, Germany, 22-24 Jul, 2011.
                </div>
          		</div> <!-- item end -->          		          		          		
          		
            </div> <!-- publications-wrapper end -->
         </div> <!-- twelve columns end -->

         <!--  Popup
	      --------------------------------------------------------------- -->

         <div id="journal-01" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Late Reverberation Cancellation Using Bayesian Estimation of Multi-Channel Linear Predictors and Student's t-Source Prior</h4>
			      <p>Abstract: Multi-channel linear prediction (MCLP) can model the late reverberation in the short-time Fourier transform domain using a delayed linear predictor and the prediction residual is taken as the desired early reflection component. Traditionally, a Gaussian source model with time-dependent precision (inverse of variance) is considered for the desired signal. In this paper, we propose a Student's t-distribution model for the desired signal, which is realized as a Gaussian source with a Gamma distributed precision. Further, since the choice of a proper MCLP order is critical, we also incorporate a Gaussian distribution prior for the prediction coefficients and a higher order. We consider a batch estimation scenario and develop variational Bayes expectation maximization (VBEM) algorithm for joint posterior inference and hyper-parameter estimation. This has lead to more accurate and robust estimation of the late reverb component and hence its cancellation, benefitting the desired residual signal estimation. Along with these stochastic models, we formulate single channel output (MISO) and multi channel output (MIMO) schemes using shared priors for the desired signal precision and the estimated MCLP coefficients at each microphone. Experiments using real room impulse responses show improved late reverberation suppression with the proposed VBEM approach over the traditional methods, for different room conditions. Additionally, we achieve a sparse coefficient vector for the MCLP avoiding the criticality of manually choosing the model order. The MIMO formulation is easily extended to include spatial filtering of the enhanced signals, which further improves the estimation of the desired signal.</p>
		      </div>

            <div class="link-box">
               <a href="https://ieeexplore.ieee.org/abstract/document/8675317" target="_blank">Know more</a> | 
		         <a href="bayesian_lp.html" target="_blank">Demo</a> | <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- journal-01 End -->	      	      

         <div id="conference-13" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Clean speech AE-DNN PSD constraint for MCLP based reverberant speech enhancement</h4>
			      <p>Abstract: Blind inverse filtering using multi-channel linear prediction (MCLP) in short-time Fourier transform (STFT) domain is an effective means to enhance reverberant speech. Traditionally, a speech power spectral density (PSD) weighted prediction error (WPE) minimization approach is used to estimate the prediction filters, independently in each frequency bin. The method is sensitive to the estimation of desired signal PSD. In this paper, we propose an auto-encoder (AE) deep neural network (DNN) based constraint for the estimation of desired signal PSD. An auto encoder trained on clean speech STFT coefficients is used as the prior to non-linearly map the natural speech PSD. We explore two different architectures for the auto-encoder: (i) fully-connected (FC) feed-forward, and (ii) recurrent long short-term memory (LSTM) architecture. Experiments using real room impulse responses show that the LSTM-DNN based PSD estimate performs better than the traditional methods for reverberant signal enhancement.</p>
		      </div>

            <div class="link-box">
            	<a href="https://arxiv.org/abs/1812.01346" target="_blank"> Know more </a> | 
				<a href="lstmMCLP.html" target="_blank"> Demo </a> | 
		        <a class="popup-modal-dismiss">Close</a>
            </div>
	      </div><!-- conference-12 End -->	  

         <div id="conference-12" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Ad-hoc mobile array based audio segmentation using latent variable stochastic model</h4>
			      <p>Abstract: Segmentation/diarization of audio recordings using a network of ad-hoc mobile arrays and the spatial information gathered is a part of acoustic scene analysis. Because of deploying ad-hoc mobile devices, synchronous recording is assumed only at each array node and a gross feature level synchrony across different nodes of the network. We compute spatial features at each node in a distributed manner without the overhead of signal data	aggregation between mobile devices. The spatial features are then modeled jointly using a Dirichlet mixture model, and the posterior probabilities of the mixture components are used to derive the segmentation information. Experiments on real life recordings in a reverberant room using a network of randomly placed mobile phones has shown a diarization error rate of less than 14% even with overlapped talkers.</p>
		      </div>

            <div class="link-box">
            	<a href="https://arxiv.org/abs/1810.13109" target="_blank"> Know more </a> | 
            	<a href="mDiar.html" target="_blank"> Demo </a> | 
		        <a class="popup-modal-dismiss">Close</a>
            </div>
	      </div><!-- conference-12 End -->	  

         <div id="conference-12" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Ad-hoc mobile array based audio segmentation using latent variable stochastic model</h4>
			      <p>Abstract: Segmentation/diarization of audio recordings using a network of ad-hoc mobile arrays and the spatial information gathered is a part of acoustic scene analysis. Because of deploying ad-hoc mobile devices, synchronous recording is assumed only at each array node and a gross feature level synchrony across different nodes of the network. We compute spatial features at each node in a distributed manner without the overhead of signal data aggregation between mobile devices. The spatial features are then modeled jointly using a Dirichlet mixture model, and the posterior probabilities of the mixture components are used to derive the segmentation information. Experiments on real life recordings in a reverberant room using a network of randomly placed mobile phones has shown a diarization error rate of less than 14% even with overlapped talkers.</p>
		      </div>

            <div class="link-box">
            	<a href="https://arxiv.org/abs/1810.13109" target="_blank"> Know more </a>
		         <a class="popup-modal-dismiss">Close</a>
            </div>
	      </div><!-- conference-12 End -->	  

         <div id="conference-11" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Comparison of low-dimension speech segment embeddings: Application to speaker diarization</h4>
			      <p>Abstract: Segment clustering is a crucial step in unsupervised speaker diarization. Bottom-up 
			      approaches, such as, hierarchical agglomerative clustering technique are used traditionally for 
			      segment clustering. In this paper, we consider the top-down approach to clustering, in which a speaker 
			      sensitive, low-dimensional representation of segments (speaker space) is obtained first, followed by 
			      Gaussian mixture model (GMM) based clustering. We explore three methods of obtaining the low dimension 
			      segment representation: (i) multi-dimensional scaling (MDS) based on segment to segment stochastic 
			      distances; (ii) traditional principal component analysis (PCA), and (iii) factor analysis (i-vectors), 
			      of GMM mean super-vectors. We found that, MDS based embeddings result in better representation and hence 
			      result in better diarization performance compared to PCA and even i-vector embeddings.</p>
		      </div>

            <div class="link-box">
            	<!--<a href="https://ieeexplore.ieee.org/document/8521273" target="_blank"> Know more </a>-->					
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-11 End -->	  

         <!--  Popup
	      --------------------------------------------------------------- -->
         <div id="conference-10" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Linear Prediction Based Diffuse Signal Estimation for Blind Microphone Geometry Calibration</h4>
			      <p>Abstract: Spatial cross coherence function between two locations in a diffuse sound field is a function 
			      of the distance between them. Earlier approaches to microphone geometry calibration utilizing this 
			      property assumed the presence of ambient noise sources. In this paper, we consider the geometry estimation 
			      using a single acoustic source (not noise) and show that late reverberation (diffuse signal) estimation 
			      using multi-channel linear prediction (MCLP) provides a computationally efficient solution. The idea behind 
			      this is that, the component of a reverberant signal corresponding to late reflections satisfies the 
			      diffuse sound field properties, which we exploit for distance estimation between microphone pairs. 
			      MCLP of short-time Fourier transform (STFT) coefficients is used to decompose each microphone signal into 
			      early and late reflection components. Cross coherence computed between the separated late reflection 
			      components is then used for pair-wise microphone distance estimation. Multidimensional scaling (MDS) is 
			      then used to estimate the geometry of the microphones from pair-wise distance measurements. We show that, 
			      higher reverberation, though detrimental to signal estimation, can aid in microphone geometry estimation. 
			      Estimated position error of less then $2~cm$ is achieved using the proposed approach for real microphone 
			      recorded signals.</p>
		      </div>

            <div class="link-box">
            	<a href="https://ieeexplore.ieee.org/document/8521273" target="_blank"> Know more </a> | 
					<a href="self_localization.html" target="_blank">Demo</a> | 
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-10 End -->	  


         <!--  Popup
	      --------------------------------------------------------------- -->
         <div id="conference-09" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Robust offline trained neural network for TDOA based sound source localization</h4>
			      <p>Abstract: Passive sound source localization (SSL) using time-difference-of-arrival (TDOA) 
			      measurements is a non-linear inversion problem. In this paper, a data-driven approach to SSL 
			      using TDOA measurements is considered. A neural network (NN) is viewed as an architecture 
			      constrained non-linear function, with its parameters learnt from the training data. We consider 
			      a three layer neural network with TDOA measurements between pairs of microphones as input features 
			      and source location in the Cartesian coordinate system as output. Experimentally, we show that, NN 
			      trained even on noise-less TDOA measurements can achieve good performance for noisy TDOA inputs also.
					These performances are better than the traditional spherical interpolation (SI) method. We show that 
					the NN trained offline using simulated TDOA measurements, performs better than the SI method, on 
					real-life speech signals in a simulated enclosure.</p>
		      </div>

            <div class="link-box">
            	<a href="https://ieeexplore.ieee.org/document/8600013" target="_blank"> Know more </a>
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-09 End -->	  

         <!--  Popup
	      --------------------------------------------------------------- -->
         <div id="conference-08" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Non-Linear Filtering for Feature Enhancement of Reverberant Speech</h4>
			      <p>Abstract: Speaker identification implemented on a mobile robot is a challenging problem 
			      because of varying reverberant environments which the robot encounters while in motion. 
			      The performance of a typical speaker identification system degrades significantly in reverberant 
			      environments. The degradation in performance is mainly due to the conventional feature being not 
			      robust to change in reverberant condition. In this paper, we present a non-linear filter based mel 
			      frequency cepstral coefficient (MFCC) feature extraction, which is more robust to changes in 
			      reverberant conditions. This feature extraction method is a two stage operation and is applied on 
			      the spectrogram of the speech signal. The first stage suppresses the frequency spread due to 
			      reverberation within each frame and in the second stage, reverberation effect across the frames 
			      is suppressed. The performance is evaluated by the GMM-UBM based identifier built and tested with 
			      conventional MFCC feature vectors and with the non-linear filter based MFCC feature vectors. 
			      We show that, the identification accuracy of GMM-UBM based identifier with non-linear filter based 
			      MFCC feature vectors is better than that of conventional MFCC feature vectors.</p>
		      </div>

            <div class="link-box">
            	<a href="http://ieeexplore.ieee.org/document/8228150/" target="_blank">Know more</a>
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-08 End -->	      	      	      
	      

         <div id="conference-07" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Mel-Scale Sub-band Modelling for Perceptually improved Time-Scale Modification of Speech and Audio Signals</h4>
			      <p>Abstract: Reducing artifacts of time-scale or pitch-scale modified speech is a classical problem and we 
			      address the same using time-varying signal models. We choose the AM-FM decomposition, i.e., instantaneous 
			      amplitude (IA) and instantaneous phase (IP), of narrow band signals to compose the overall speech signal. 
			      To suit perceptual aspects of the scale modified signal, we choose AM-FM decomposition of mel-scale sub-band 
			      filtered speech. The required scale modification is then applied to the multi-band IA and IP signals, 
			      individually. Experiments with time-scaling is found to preserve the spectral content, pitch, and also 
			      preserve the temporal structure. Listening test results for speech, and music (solo, and polyphonic) signals 
			      are compared with "phase vocoder with identity phase locking" and "harmonic-percussive separation (HP)" 
			      based time-scaling methods. The performance of mel-sub-band AM-FM shows a significant improvement in reduced 
			      reverberation-like perception (also referred as "phasiness"), and preserving the localized aspect of 
			      transients, in both speech and music signals. We also show the effectiveness of the new technique for 
			      non-uniform time-scale and pitch-scale modification.</p>
		      </div>

            <div class="link-box">            
 					<a href="http://ieeexplore.ieee.org/document/8077073/" target="_blank">Know more</a> 
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-07 End -->	      	      


         <div id="letters-01" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Joint Bayesian Estimation of Time-Varying LP Parameters and Excitation for Speech</h4>
			      <p>Abstract: We consider the joint estimation of time-varying linear prediction (TVLP) 
			      filter coefficients and the excitation signal parameters for the analysis of long-term 
			      speech segments. Traditional approaches to TVLP estimation assume linear expansion of 
			      the coefficients in a set of known basis functions only. But, excitation signal is also 
			      time-varying, which affects the estimation of TVLP filter parameters. In this paper, we 
			      propose a Bayesian approach, to incorporate the nature of excitation signal and also adapt 
			      regularization of the filter parameters. Since the order of the system is not known a-priori, 
			      we formulate a Gaussian prior for the filter parameters, and the excitation signal is modeled 
			      as Gaussian with time-varying Gamma distributed precision. We develop an iterative algorithm 
			      for the maximum-likelihood (ML) estimation of the posterior distribution of filter parameters 
			      and the time-varying precision of the excitation signal, along with the parameters of the prior 
			      distribution. We show that the proposed method adapts to different types of excitation signals 
			      in speech, and also the time-varying system with unknown model order. The spectral modeling 
			      performance for synthetic speech-like signals, quantified using the absolute spectral difference 
			      (SPDIFF) shows that the proposed method estimates the system function more accurately compared 
			      to several of the traditional methods.</p>
		      </div>

            <div class="link-box">
               <a href="http://ieeexplore.ieee.org/document/7839300/" target="_blank">Know more</a> 
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- letters-01 End -->	      	      
	      
         <div id="conference-06" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Feature Selection and Model Optimization for Semi-supervised Speaker Spotting</h4>
			      <p>Abstract: We explore, experimentally, feature selection and optimization of stochastic 
			      model parameters for the problem of speaker spotting. Based on an initially identified segment 
			      of speech of a speaker, an iterative model refinement method is developed along with a latent 
			      variable mixture model so that segments of the same speaker are identified in a long speech record. 
			      It is found that a GMM with moderate number of mixtures is better suited for the task than a large 
			      number mixture model as used in speaker identification. Similarly, a PCA based low-dimensional 
			      projection of MFCC based feature vector provides better performance. We show that about 6 seconds of 
			      initially identified speaker data is sufficient to achieve > 90% performance of speaker segment 
			      identification</p>
		      </div>

            <div class="link-box">
               <a href="http://www.eurasip.org/Proceedings/Eusipco/Eusipco2016/papers/1570256557.pdf" target="_blank">Know more</a> 
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-06 End -->	      
	      	      
	      
         <div id="conference-05" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Successive Approximation Algorithm for LPC Estimation Using Sparse Residual Constraint</h4>
			      <p>Abstract: Estimation of linear prediction coefficients under the sparsity constraint of the 
			      prediction residue, is a modification of the traditional minimum mean square error linear predictor 
			      formulation, which accounts for the impulse nature of the residual signal for voiced speech signals. 
			      This is solved using the 1-norm minimization approach under sparsity constraints. In this paper, we 
			      develop a successive approximation algorithm for estimating the linear predictor coefficients and the 
			      sparse residual signal. We illustrate the usefulness of the proposed approach using synthetic, and also 
			      real speech examples. Experimental results in a multi-pulse based analysis-synthesis show that the 
			      proposed approach can provide better perceptual quality speech reconstruction than the orthogonal 
			      matching pursuit based algorithm, with computational time much lower than convex optimization based 
			      techniques.</p>
		      </div>

            <div class="link-box">
               <a href="http://dx.doi.org/10.1109/NCC.2015.7084883" target="_blank">Know more</a> 
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-05 End -->	      
	      	      
         <div id="conference-04" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Time Varying Linear Prediction using Sparsity Constraints</h4>
			      <p>Abstract: Time-varying linear prediction has been studied in the context of speech signals, in which the 
			      auto-regressive (AR) coefficients of the system function are modeled as a linear combination of a 
			      set of known bases. Traditionally, least squares minimization is used for the estimation of model 
			      parameters of the system. Motivated by the sparse nature of the excitation signal for voiced sounds, 
			      we explore the time-varying linear prediction modeling of speech signals using sparsity constraints. 
			      Parameter estimation is posed as a 0-norm minimization problem. The re-weighted 1-norm minimization 
			      technique is used to estimate the model parameters. We show that for sparsely excited time-varying 
			      systems, the formulation models the underlying system function better than the least squares error 
			      minimization approach. Evaluation with synthetic and real speech examples show that the estimated 
			      model parameters track the formant trajectories closer than the least squares approach.</p>
		      </div>

            <div class="link-box">
               <a href="http://dx.doi.org/10.1109/ICASSP.2014.6854814" target="_blank">Know more</a> 
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-04 End -->	      
	      
         <div id="conference-03" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Joint Pitch-Analysis Formant-Synthesis framework for CS recovery of speech</h4>
			      <p>Abstract: A joint analysis-synthesis framework is developed for the compressive sensing recovery 
			      of speech signals. The signal is assumed to be sparse in the residual domain with the linear prediction 
			      filter used as the sparse transformation. Importantly this transform is not known apriori, since estimating 
			      the predictor filter requires the knowledge of the signal. Two prediction filters, one comb filter for 
			      pitch and another all pole formant filter are needed to induce maximum sparsity. An iterative method is 
			      proposed for the estimation of the prediction filters and the signal itself. Formant prediction filter is 
			      used as the synthesis transform, while the pitch filter is used to model the periodicity in the residual 
			      excitation signal.</p>
		      </div>

            <div class="link-box">
               <a href="http://www.isca-speech.org/archive/interspeech_2012/i12_0947.html" target="_blank">Know more</a>
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-03 End -->
	      	      
         <div id="conference-02" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Time-varying signal adaptive transform and IHT recovery of compressive sensed speech</h4>
			      <p>Abstract: Compressive Sensing (CS) signal recovery has been formulated for signals sparse in a 
			      known linear transform domain. We consider the scenario in which the transformation is unknown and 
			      the goal is to estimate the transform as well as the sparse signal from just the CS measurements. 
			      Specifically, we consider the speech signal as the output of a time-varying AR process, as in the 
			      linear system model of speech production, with the excitation being sparse. We propose an iterative 
			      algorithm to estimate both the system impulse response and the excitation signal from the CS measurements. 
			      We show that the proposed algorithm, in conjunction with a modified iterative hard thresholding, is able to 
			      estimate the signal adaptive transform accurately, leading to much higher quality signal reconstruction than 
			      the codebook based matching pursuit approach. The estimated time-varying transform is better than a 256 size 
			      codebook estimated from original speech. Thus, we are able to get near “toll quality” speech reconstruction 
			      from sub-Nyquist rate CS measurements.</p>
		      </div>

            <div class="link-box">
               <a href="http://www.isca-speech.org/archive/interspeech_2011/i11_0073.html" target="_blank">Know more</a>
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-02 End -->
	      
         <div id="conference-01" class="popup-modal mfp-hide">

		      <img class="scale-with-grid" src="content/iisclogo.jpg" alt="" />

		      <div class="description-box">
			      <h4>Compressive Sensing for Music signals: Comparison of transforms with coherent dictionaries</h4>
			      <p>Compressive Sensing (CS) is a new sensing paradigm which permits sampling of a signal at its intrinsic 
			      information rate which could be much lower than Nyquist rate, while guaranteeing good quality reconstruction 
			      for signals sparse in a linear transform domain. We explore the application of CS formulation to music signals. 
			      Since music signals comprise of both tonal and transient nature, we examine several transforms such as discrete cosine transform (DCT), discrete wavelet transform (DWT), Fourier basis and also 
			      non-orthogonal warped transforms to explore the effectiveness of CS theory and the reconstruction algorithms. 
			      We show that for a given sparsity level DCT, overcomplete, and warped Fourier dictionaries result in better 
			      reconstruction, and warped Fourier dictionary gives perceptually better reconstruction. “MUSHRA” test results 
			      show that a moderate quality reconstruction is possible with about half the Nyquist sampling.</p>
		      </div>

            <div class="link-box">
               <a href="http://www.aes.org/e-lib/browse.cfm?elib=15946" target="_blank">Know more</a>
		         <a class="popup-modal-dismiss">Close</a>
            </div>

	      </div><!-- conference-01 End -->


      </div> <!-- row End -->

   </section> <!-- publications Section End-->


   <!-- News Section
   ================================================== -->
   <section id="news">

      <div class="text-container">
         <div class="row">
         <div class="twelve columns collapsed">
               	<h1>NEWS</h1>
               	<hr>
                  <ul style="list-style-type:disc">														
                     <li>
                        <blockquote>
                           <p> I was awarded TCS fellowship. More details can be found <a href="http://ece.iisc.ernet.in/index.php/news/165-srikanth-raj-awarded-the-2015-16-tcs-research-scholarship">here</a> 
                           </p>
                        </blockquote>
                     </li> <!-- slide ends -->
                  </ul>
         
         </div>
         </div> <!-- row ends -->

       </div>  <!-- text-container ends -->

   </section> <!-- news Section End-->


   <!-- footer
   ================================================== -->
   <footer>
      <div class="row">
         <div class="twelve columns">
            <ul class="copyright">
               <li>&copy; Copyright 2016 Srikanth</li>
               <li>Template adapted from <a title="Styleshout" href="http://www.styleshout.com/">Styleshout</a></li>   
            </ul>
         </div>
         <div id="go-top"><a class="smoothscroll" title="Back to Top" href="#home"><i class="icon-up-open"></i></a></div>
      </div>
   </footer> <!-- Footer End-->

   <!-- Java Script
   ================================================== -->
   <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
   <script>window.jQuery || document.write('<script src="js/jquery-1.10.2.min.js"><\/script>')</script>
   <script type="text/javascript" src="js/jquery-migrate-1.2.1.min.js"></script>

   <script src="js/jquery.flexslider.js"></script>
   <script src="js/waypoints.js"></script>
   <script src="js/jquery.fittext.js"></script>
   <script src="js/magnific-popup.js"></script>
   <script src="js/init.js"></script>

</body>

</html>
